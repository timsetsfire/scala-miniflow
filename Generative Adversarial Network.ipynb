{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter nbextension enable beakerx --py --sys-prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d7ac1f7-61f9-48aa-8932-b436459ded41",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%classpath add mvn org.nd4j nd4j-native-platform 0.7.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e333c9-9170-4cee-95d1-0c5a7197d438",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%classpath add mvn org.nd4j nd4s_2.11 0.7.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69087cee-c762-4223-9005-b378e9c12337",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%classpath add mvn org.vegas-viz vegas_2.11 0.3.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e9d4137-1403-4e61-95d8-cdb3c5e868ce",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%classpath add jar target/scala-2.11/scala-miniflow_2.11-0.1.0-SNAPSHOT.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks\n",
    "\n",
    "The objective today is to use a neural network library written from scratch to create a generative adversarial network from which we will create data that resembles the ever so popular MNIST data set.  \n",
    "\n",
    "First let's get a few questions out of the way\n",
    "\n",
    "## Why?\n",
    "\n",
    "1.  Scala - because it is a great language\n",
    "2.  ND4J - it is the main linear algebra library for DL4J (Deep learning 4 Java).  The author's objective was to shorten the gap between JVM languages and Numpy or Matlab\n",
    "3.  From Scratch - it is challenging a.f. and a lot of fun.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Feed Forward Neural networks\n",
    "\n",
    "Here we will begin to get into how we will model our neural network framework. Per Deep Learning by Goodfellow, the feedforward neural network is called such because\n",
    "\n",
    "* (feedforward) information flows through the function being evaluated from the input $x$, through intermediate computations used to define $f$, and finally to the output $y$. We are not considering any feedback connections\n",
    "* (network) They are typically represented by composing together many different functions. The model is associated with a directy acyclic graph describing how functions are componsed together.\n",
    "* (neural) The models are loosly inspired by neuroscience.  \n",
    "\n",
    "Albert did such a great job on from scratch that I'm not going to bother.  \n",
    "\n",
    "Our Node will\n",
    "* take as arguments, incoming nodes\n",
    "* have a method which captures outbound nodes\n",
    "* has a forward method (feed forward)\n",
    "* has a backward method (for back propagation)\n",
    "\n",
    "We'll demonstrate a simple example of using the framework to estimate a linear regression on the boston dataset, and then we'll jump into the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.nd4j.linalg.factory.Nd4j\r\n",
       "import org.nd4s.Implicits._\r\n",
       "import org.nd4j.linalg.api.ndarray.INDArray\r\n",
       "import org.nd4s.Implicits._\r\n",
       "import com.github.timsetsfire.nn.node._\r\n",
       "import com.github.timsetsfire.nn.activation._\r\n",
       "import com.github.timsetsfire.nn.costfunctions._\r\n",
       "import com.github.timsetsfire.nn.regularization._\r\n",
       "import com.github.timsetsfire.nn.optimize._\r\n",
       "import com.github.timsetsfire.nn.graph._\n"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.nd4j.linalg.factory.Nd4j\n",
    "import org.nd4s.Implicits._\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import org.nd4s.Implicits._\n",
    "\n",
    "import com.github.timsetsfire.nn.node._\n",
    "import com.github.timsetsfire.nn.activation._\n",
    "import com.github.timsetsfire.nn.costfunctions._\n",
    "import com.github.timsetsfire.nn.regularization._\n",
    "import com.github.timsetsfire.nn.optimize._\n",
    "import com.github.timsetsfire.nn.graph._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{weights}@44b16a2e\n",
      "{features}@387742e3\n",
      "{bias}@208dc62a\n",
      "{target}@3cbb1c7e\n",
      "{prediction}@70e79f7a\n",
      "MSE@7910b546\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x = new Input()  \n",
    "x.setName(\"features\")\n",
    "val y = new Input()  \n",
    "y.setName(\"target\")\n",
    "val w = new Variable()\n",
    "w.setName(\"weights\")\n",
    "val b = new Variable()\n",
    "b.setName(\"bias\")\n",
    "val yhat = new Linear(x,w,b)\n",
    "//val yhat = (x * w) + b\n",
    "yhat.setName(\"prediction\")\n",
    "val mse = new MSE(y, yhat)\n",
    "\n",
    "val regression = topologicalSort{ \n",
    "    buildGraph(mse) \n",
    "}\n",
    "\n",
    "regression.foreach(println)\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x_ = Nd4j.readNumpy(\"resources/boston_x.csv\", \",\")\n",
    "val y_ = Nd4j.readNumpy(\"resources/boston_y.csv\", \",\")\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 506, columns: 13"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(xrows,nfeatures) = x_.shape\n",
    "print(s\"rows: $xrows, columns: $nfeatures\")\n",
    "\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Initilization\n",
    "\n",
    "We need to initalize the bias and the weights.  For out purposes, we'll just set them equal to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.forward(Nd4j.zeros(1,1))\n",
    "w.forward(Nd4j.zeros(13,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// standardize data \n",
    "val xs_ = x_.subRowVector(x_.mean(0)).divRowVector( x_.std(0))\n",
    "val ys_ = y_.subRowVector(y_.mean(0)).divRowVector( y_.std(0))\n",
    "val data = Nd4j.concat(1, ys_, xs_);\n",
    "val epochs = 500\n",
    "val batchSize = 100\n",
    "val stepsPerEpoch = xrows / batchSize\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.5307063281536102\n",
      "Epoch: 50, Loss: 0.27538190484046937\n",
      "Epoch: 100, Loss: 0.2489509642124176\n",
      "Epoch: 150, Loss: 0.3016616731882095\n",
      "Epoch: 200, Loss: 0.29843970835208894\n",
      "Epoch: 250, Loss: 0.280102527141571\n",
      "Epoch: 300, Loss: 0.21600520610809326\n",
      "Epoch: 350, Loss: 0.2709260553121567\n",
      "Epoch: 400, Loss: 0.2555361956357956\n",
      "Epoch: 450, Loss: 0.2606124997138977\n",
      "Epoch: 500, Loss: 0.29919455349445345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sgd = new GradientDescent(regression, learningRate = 0.1)\n",
    "for(epoch <- 0 to epochs) {\n",
    "  var loss = 0d\n",
    "  for(j <- 0 until stepsPerEpoch) {\n",
    "\n",
    "    Nd4j.shuffle(data, 1)\n",
    "\n",
    "    val feedDict: Map[Node, Any] = Map(\n",
    "      x -> data.getColumns( (1 to nfeatures):_*).getRows((0 until batchSize):_*),\n",
    "      y -> data.getColumn(0).getRows((0 until batchSize):_*)\n",
    "    )\n",
    "\n",
    "    sgd.optimize(feedDict)\n",
    "\n",
    "    loss += mse.value(0,0)\n",
    "  }\n",
    "  if(epoch % 50 == 0)  println(s\"Epoch: ${epoch}, Loss: ${loss/stepsPerEpoch.toDouble}\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.forward(xs_)\n",
    "y.forward(ys_)\n",
    "regression.foreach( _.forward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final cost: 0.27\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(s\"final cost: ${mse.value}\")\n",
    "\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import com.github.timsetsfire.nn.node._\r\n",
       "import com.github.timsetsfire.nn.activation._\r\n",
       "import com.github.timsetsfire.nn.costfunctions._\r\n",
       "import com.github.timsetsfire.nn.batchnormalization._\r\n",
       "import com.github.timsetsfire.nn.regularization.Dropout\r\n",
       "import com.github.timsetsfire.nn.optimize._\r\n",
       "import com.github.timsetsfire.nn.graph._\r\n",
       "import scala.util.Try\r\n",
       "import org.nd4j.linalg.factory.Nd4j\r\n",
       "import org.nd4j.linalg.api.ndarray.INDArray\r\n",
       "import org.nd4j.linalg.ops.transforms.Transforms.{sigmoid, exp, log, pow, sqrt}\r\n",
       "import org.nd4s.Implicits._\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.github.timsetsfire.nn.node._\n",
    "import com.github.timsetsfire.nn.activation._\n",
    "import com.github.timsetsfire.nn.costfunctions._\n",
    "import com.github.timsetsfire.nn.batchnormalization._\n",
    "import com.github.timsetsfire.nn.regularization.Dropout\n",
    "import com.github.timsetsfire.nn.optimize._\n",
    "import com.github.timsetsfire.nn.graph._\n",
    "\n",
    "import scala.util.Try\n",
    "import org.nd4j.linalg.factory.Nd4j\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import org.nd4j.linalg.ops.transforms.Transforms.{sigmoid,exp,log,pow,sqrt}\n",
    "import org.nd4s.Implicits._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get and Import Data \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `curl -s https://pjreddie.com/media/files/mnist_test.csv > resources/mnist_test.csv` in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x_ = Nd4j.readNumpy(\"resources/mnist_test.csv\", \",\").getColumns( (1 until 785):_*).div(255d)\n",
    "\n",
    "\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "\n",
    "For inputs, we'll have fake data, real data, and labels.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val images = new Input()\n",
    "images.setName(\"images\")\n",
    "val labels = new Input()\n",
    "labels.setName(\"labels\")\n",
    "val noise = new Input()  // noise is used to generate fake images\n",
    "noise.setName(\"noise\")\n",
    "val fakeLabels = new Input()\n",
    "fakeLabels.setName(\"fake_labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Network\n",
    "\n",
    "This is the network that will generate fake data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val h1Generator= LeakyReLU(noise, (100,128), 0.2)\n",
    "h1Generator.setName(\"generator_hidden1\")\n",
    "\n",
    "val h2Generator= LeakyReLU(h1Generator, (128, 256), 0.2)\n",
    "h2Generator.setName(\"generator_hidden2\")\n",
    "\n",
    "val h3Generator= LeakyReLU(h2Generator, (256, 512), 0.2)\n",
    "h3Generator.setName(\"generator_hidden3\")\n",
    "\n",
    "val fakeImages = Sigmoid(h3Generator, (512,784))\n",
    "fakeImages.setName(\"fake_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Variable@1df7920a, Variable@1c0880f1, Variable@10fbbcca, Variable@45a08d47, Variable@6db8eb0e, Variable@223a12bd, Variable@2a629dd0, Variable@8707d6f]]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val generatorNetwork = buildGraph(fakeImages)\n",
    "val generator = topologicalSort(generatorNetwork)\n",
    "val generatorTrainables = generator.filter{ _.getClass.getSimpleName == \"Variable\" }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val h1Discrim = LeakyReLU(images, (784,256), 0.1)\n",
    "h1Discrim.setName(\"discriminator_hidden_layer1\")\n",
    "\n",
    "val d1 = new Dropout(h1Discrim, 0.20)\n",
    "d1.setName(\"dropout_h1_layer\")\n",
    "\n",
    "val h2Discrim = LeakyReLU(d1, (256,64), 0.1)\n",
    "h2Discrim.setName(\"discriminator_hidden_layer2\")\n",
    "\n",
    "val d2 = new Dropout(h2Discrim, 0.20)\n",
    "d2.setName(\"dropout_h2_layer\")\n",
    "\n",
    "val h3Discrim = LeakyReLU(d2, (64,16), 0.1)\n",
    "h3Discrim.setName(\"discriminator_hidden_layer3\")\n",
    "\n",
    "val logits = Linear(h3Discrim, (16, 1))\n",
    "logits.setName(\"discriminator_logits\")\n",
    "\n",
    "val cost = new BceWithLogits(labels, logits)\n",
    "cost.setName(\"cost_function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Variable@45628127, Variable@707c13f8, Variable@7b81667, Variable@32cc16f8, Variable@4b5b1bad, Variable@c97378f, Variable@5591bbbe, Variable@53dbbd83]]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val discriminatorNetwork = buildGraph(cost)\n",
    "val discriminator = topologicalSort(discriminatorNetwork)\n",
    "val discriminatorTrainables = discriminator.filter{ _.getClass.getSimpleName == \"Variable\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// initialize generator and discriminator parameters\n",
    "discriminatorTrainables.foreach{ node =>\n",
    "    val (m,n) = node.size\n",
    "    node.value = Nd4j.randn(m.asInstanceOf[Int], n.asInstanceOf[Int]) * math.sqrt(3/(m.asInstanceOf[Int].toDouble + n.asInstanceOf[Int].toDouble))\n",
    "  }\n",
    "\n",
    "// initialize generator and discriminator\n",
    "generatorTrainables.foreach{ node =>\n",
    "    val (m,n) = node.size\n",
    "    node.value = Nd4j.randn(m.asInstanceOf[Int], n.asInstanceOf[Int]) * math.sqrt(3/(m.asInstanceOf[Int].toDouble + n.asInstanceOf[Int].toDouble))\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up first and second moment maps for Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(xrows, xcols) = x_.shape\n",
    "val batchSize = 128\n",
    "val stepsPerEpoch = xrows / batchSize\n",
    "\n",
    "val firstMomentGenerator = generatorTrainables.map{ i => (i, Nd4j.zerosLike(i.value))}.toMap\n",
    "val secondMomentGenerator = generatorTrainables.map{ i => (i, Nd4j.zerosLike(i.value))}.toMap\n",
    "val firstMomentDiscriminator = discriminatorTrainables.map{ i => (i, Nd4j.zerosLike(i.value))}.toMap\n",
    "val secondMomentDiscriminator = discriminatorTrainables.map{ i => (i, Nd4j.zerosLike(i.value))}.toMap\n",
    "val t = new java.util.concurrent.atomic.AtomicInteger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "setDropoutTraining: (n: com.github.timsetsfire.nn.node.Node, training: Boolean)Unit\n"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def setDropoutTraining(n: Node, training: Boolean = false): Unit = {\n",
    "  n.asInstanceOf[Dropout[Node]].train = training\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0E-8"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var stepSize: Double = 0.002 // 0.001 default\n",
    "val beta1: Double = 0.2  // 0.9 default\n",
    "val beta2: Double = 0.999  // 0.999 default\n",
    "val delta: Double = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "val noiseDataForPicture = Nd4j.rand(16,100).mul(2).sub(1)\n",
    "OutputCell.HIDDEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\tdiscriminator -> loss: 0.703\tgenerator -> loss: 0.791\n",
      "epoch: 10\tdiscriminator -> loss: 0.280\tgenerator -> loss: 2.824\n",
      "epoch: 20\tdiscriminator -> loss: 0.061\tgenerator -> loss: 3.083\n",
      "epoch: 30\tdiscriminator -> loss: 0.466\tgenerator -> loss: 1.243\n",
      "epoch: 40\tdiscriminator -> loss: 0.537\tgenerator -> loss: 0.976\n",
      "epoch: 50\tdiscriminator -> loss: 0.570\tgenerator -> loss: 0.908\n",
      "epoch: 60\tdiscriminator -> loss: 0.565\tgenerator -> loss: 0.961\n",
      "epoch: 70\tdiscriminator -> loss: 0.538\tgenerator -> loss: 1.193\n",
      "epoch: 80\tdiscriminator -> loss: 0.422\tgenerator -> loss: 1.655\n",
      "epoch: 90\tdiscriminator -> loss: 0.270\tgenerator -> loss: 2.307\n",
      "epoch: 100\tdiscriminator -> loss: 0.319\tgenerator -> loss: 2.895\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for(epoch <- 0 to 20) {\n",
    "\n",
    "      var loss = 0d\n",
    "      var genCost = 0d\n",
    "      var n = 0d\n",
    "      for(steps <- 0 to stepsPerEpoch) {\n",
    "\n",
    "        t.addAndGet(1)\n",
    "\n",
    "        val noiseData = Nd4j.rand(batchSize,100).mul(2).sub(1)\n",
    "        val fakeLabelData = Nd4j.ones(batchSize, 1)\n",
    "\n",
    "        val generatorFeedDict: Map[Node, INDArray] = Map(\n",
    "          noise -> noiseData,\n",
    "          fakeLabels -> fakeLabelData\n",
    "        )\n",
    "\n",
    "        // generator\n",
    "        discriminator.filter{ _.getClass.getSimpleName == \"Dropout\"}.foreach(d => setDropoutTraining(d, false))\n",
    "        generatorFeedDict.foreach{ case (n, v) => n.forward(v)}\n",
    "        generator.foreach(_.forward())\n",
    "        images.forward(fakeImages.value)\n",
    "        labels.forward(fakeLabels.value)\n",
    "        discriminator.foreach(_.forward())\n",
    "        discriminator.reverse.foreach(_.backward())\n",
    "        //fakeImages.gradients(fakeImages) = images.gradients(images)\n",
    "        fakeImages.backward(images.gradients(images).dup)\n",
    "        generator.reverse.tail.foreach(_.backward())\n",
    "        // still need to update parameters of generator\n",
    "        genCost += (cost.value.sumT*batchSize)\n",
    "\n",
    "        for( n <- generatorTrainables) {\n",
    "          firstMomentGenerator(n).muli(beta1).addi(n.gradients(n).mul(1 - beta1))\n",
    "          secondMomentGenerator(n).muli(beta2).addi( pow(n.gradients(n),2).mul(1 - beta2))\n",
    "          val fhat = firstMomentGenerator(n).div(1 - math.pow(beta1, t.get))\n",
    "          val shat = secondMomentGenerator(n).div(1 - math.pow(beta2, t.get))\n",
    "          n.value.addi( fhat.mul(-stepSize).div(sqrt(shat).add(delta)))\n",
    "        }\n",
    "\n",
    "\n",
    "        generator.foreach(_.forward())\n",
    "        val fakeImageData = fakeImages.value\n",
    "        Nd4j.shuffle(x_,1)\n",
    "        val realImageData = x_.getRows((0 until batchSize):_*)\n",
    "        val realLabelData = Nd4j.ones(batchSize, 1)\n",
    "        val fakeLabelData0 = Nd4j.zeros(batchSize, 1)\n",
    "\n",
    "        val labelData = Nd4j.concat(0, fakeLabelData0, realLabelData)\n",
    "        val imageData = Nd4j.concat(0, fakeImageData, realImageData)\n",
    "        val discriminatorFeedDict: Map[Node, INDArray] = Map(\n",
    "          images -> imageData,\n",
    "          labels -> labelData\n",
    "        )\n",
    "\n",
    "        discriminator.filter{ _.getClass.getSimpleName == \"Dropout\"}.foreach(d => setDropoutTraining(d, true))\n",
    "        discriminatorFeedDict.foreach{ case (n, v) => n.forward(v)}\n",
    "        discriminator.foreach(_.forward())\n",
    "        discriminator.reverse.foreach(_.backward())\n",
    "\n",
    "        for( n <- discriminatorTrainables) {\n",
    "          firstMomentDiscriminator(n).muli(beta1).addi(n.gradients(n).mul(1d - beta1))\n",
    "          secondMomentDiscriminator(n).muli(beta2).addi( pow(n.gradients(n),2).mul(1d - beta2))\n",
    "          val fhat = firstMomentDiscriminator(n).div(1 - math.pow(beta1, t.get))\n",
    "          val shat = secondMomentDiscriminator(n).div(1 - math.pow(beta2, t.get))\n",
    "          n.value.addi( fhat.mul(-stepSize).div(sqrt(shat).add(delta)))\n",
    "        }\n",
    "\n",
    "        loss += ((cost.value(0,0)) * images.value.shape.apply(0))\n",
    "        n += images.value.shape.apply(0)\n",
    "      }\n",
    "      // if(epoch % 1000 == 0) stepSize /= 2d\n",
    "      if(epoch % 10 == 0) {\n",
    "        print(f\"epoch: ${epoch}\")\n",
    "        print(f\"\\tdiscriminator -> loss: ${loss / n.toDouble}%2.3f\")\n",
    "        println(f\"\\tgenerator -> loss: ${genCost / (n.toDouble/2d)}%2.3f\")\n",
    "          \n",
    "        \n",
    "      }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35af745f-fd34-4e18-a342-69fc9a505673",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "noise.forward(noiseDataForPicture)\n",
    "generator.foreach(_.forward())\n",
    "\n",
    "val d1 = fakeImages.value.getRows(1,2,3,4).data.asDouble\n",
    "val d2 = fakeImages.value.getRows(5,6,7,8).data.asDouble\n",
    "val d3 = fakeImages.value.getRows(9,10,11,12).data.asDouble\n",
    "val d4 = fakeImages.value.getRows(13,14,15,0).data.asDouble\n",
    "\n",
    "val items = 4 * 28\n",
    "\n",
    "val da = (for{ i <- 0 to items} yield d1.drop(i*28).take(28) ++ (d2.drop(i*28).take(28)) ++ (d3.drop(i*28).take(28)) ++ (d4.drop(i*28).take(28))).init \n",
    "\n",
    "\n",
    "val hm = new HeatMap\n",
    "hm.data_=( da.reverse )\n",
    "hm.color_=(GradientColor.GREEN_YELLOW_WHITE )\n",
    "\n",
    "hm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Epoch\n",
    "![epoch](resources/fig0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 10\n",
    "![epoch 100](resources/fig10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 50\n",
    "![epoch 100](resources/fig50.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 80\n",
    "![epoch 100](resources/fig80.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 100\n",
    "![epoch 100](resources/fig100.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "",
   "name": "Scala",
   "nbconverter_exporter": "",
   "version": "2.11.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
